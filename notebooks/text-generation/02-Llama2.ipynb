{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f73ff15-068b-44c7-b821-63c24fa506b9",
   "metadata": {},
   "source": [
    "<a href = \"https://www.pieriantraining.com\"><img src=\"../PT Centered Purple.png\"> </a>\n",
    "\n",
    "<em style=\"text-align:center\">Copyrighted by Pierian Training</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1133c212-7499-45b2-97ab-c95fe771cc13",
   "metadata": {},
   "source": [
    "# LLama 2\n",
    "In this notebook we are going to learn how to use AWS Bedrock to perform inference using Meta's Llama 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7d5f85-3393-426e-8000-84d6f0b6df59",
   "metadata": {},
   "source": [
    "Let's directly start by creating the bedrock runtime client:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "682fcef0-cc1f-4b88-afbc-3c04a5c2160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "bedrock_runtime = boto3.client(aws_access_key_id=\"\",\n",
    "                               aws_secret_access_key=\"\",\n",
    "                               region_name=\"us-east-1\",\n",
    "                               service_name='bedrock-runtime')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcd8f51-3d24-4673-8251-e8c58db49842",
   "metadata": {},
   "source": [
    "### Llama 2 Parameters\n",
    "Llama 2 accept the following parameters:\n",
    "\n",
    "- `prompt`: string\n",
    "  - This represents the initial text or question provided to the system.\n",
    "- `temperature`: float\n",
    "  - This controls the randomness in the text generation process. A higher temperature results in more random outputs, while a lower temperature produces more predictable text.\n",
    "- `top_p`: float\n",
    "  - This parameter, also known as \"nucleus sampling,\" controls the diversity of the generated text. It sets a threshold to include the most likely next words, cumulatively adding up to the specified probability 'P'. A lower value ignores less probable options. Set to 0 or 1.0 to disable\n",
    "- `max_gen_len`: int\n",
    "  - This specifies the maximum number of tokens that the generated text can contain.\n",
    "   \n",
    "Thus, the model expects the following jsonified request body:\n",
    "```\n",
    "{\n",
    "    \"prompt\": \"string\",\n",
    "    \"temperature\": float,\n",
    "    \"top_p\": float,\n",
    "    \"max_gen_len\": int\n",
    "}\n",
    "```\n",
    "\n",
    "Default Values:\n",
    "\n",
    "\n",
    "| Category              | Parameter            | JSON object format | Minimum | Maximum | Default |\n",
    "|-----------------------|----------------------|--------------------|---------|---------|---------|\n",
    "| Randomness and diversity | Temperature        | `temperature`      | 0       | 1       | 0.5     |\n",
    "|                       | Top P               | `top_p`            | 0       | 1       | 0.9     |\n",
    "| Length                | Max generation length| `max_gen_len`      | 1       | 2048    | 512     |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142441fe-f9c3-4187-9e3f-c3ef64e21b3a",
   "metadata": {},
   "source": [
    "Let's get started!\n",
    "Note, that the Llama we are using (`meta.llama2-13b-chat-v1`) is optimized for dialog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dc9fde4-8db3-4b7a-a163-335b7d8329f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Explain the normal distribution in the form of a medieval poem please\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a0e3b45-6157-406a-b7d0-60d4c2737f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "body = json.dumps({\n",
    "    \"prompt\": prompt,\n",
    "    \"max_gen_len\": 256,\n",
    "    \"temperature\": 0.9,  # Let's push it to be creative\n",
    "    \"top_p\": 0.9,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af38ff64-1601-4871-924c-f0351b0a2829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generation': \"\\n\\nIn days of yore, when statistics were scarce\\nAnd data was analyzed with little force\\nThere lived a knight, a statistician true\\nWhose quest was to understand the normal crew\\n\\nHe studied hard, with pen and ink\\nAnd drafted a poem to make it link\\nTo the distribution that's so normal and fine\\nThe bell-shaped curve, so pure and divine\\n\\nIn the land of statistics, where data reigns\\nThe normal distribution holds the plains\\nIt's the most common, the one we adore\\nThe distribution that we can't ignore\\n\\nThe mean, the median, the mode, they all agree\\nIn the normal distribution, they're all equal, see\\nThe data points cluster, in a curve so neat\\nThe normal distribution, a true statistical treat\\n\\nSo let us praise the normal distribution, so grand\\nThe bell-shaped curve, a wondrous land\\nWhere data points cluster, in perfect harmony\\nThe normal distribution, a true statistical show\",\n",
       " 'prompt_token_count': 14,\n",
       " 'generation_token_count': 224,\n",
       " 'stop_reason': 'stop'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = bedrock_runtime.invoke_model(body=body, modelId=\"meta.llama2-13b-chat-v1\")\n",
    "response_body = json.loads(response.get('body').read())\n",
    "response_body"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2412f462-e48c-4322-8aa6-3ea08e18b299",
   "metadata": {},
   "source": [
    "Before inspecting the poem, let's quickly look at the response structure:\n",
    "\n",
    "- `generation`: Generated text\n",
    "- `prompt_token_count`: Number of tokens in the prompt\n",
    "- `generation_token_count`: Number of tokens in the generated text\n",
    "- `stop_reason`: Reason why the response stopped\n",
    "    - `stop`: Model has finished generating text for the input prompt.\n",
    "    - `length`: Length of the tokens for the generated text exceeds the value of `max_gen_len`\n",
    " \n",
    "Let's take a look at the poem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "631b59e6-6c6d-48d1-ab0e-766909d5a397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "In days of yore, when statistics were scarce\n",
      "And data was analyzed with little force\n",
      "There lived a knight, a statistician true\n",
      "Whose quest was to understand the normal crew\n",
      "\n",
      "He studied hard, with pen and ink\n",
      "And drafted a poem to make it link\n",
      "To the distribution that's so normal and fine\n",
      "The bell-shaped curve, so pure and divine\n",
      "\n",
      "In the land of statistics, where data reigns\n",
      "The normal distribution holds the plains\n",
      "It's the most common, the one we adore\n",
      "The distribution that we can't ignore\n",
      "\n",
      "The mean, the median, the mode, they all agree\n",
      "In the normal distribution, they're all equal, see\n",
      "The data points cluster, in a curve so neat\n",
      "The normal distribution, a true statistical treat\n",
      "\n",
      "So let us praise the normal distribution, so grand\n",
      "The bell-shaped curve, a wondrous land\n",
      "Where data points cluster, in perfect harmony\n",
      "The normal distribution, a true statistical show\n"
     ]
    }
   ],
   "source": [
    "print(response_body[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46e3d4b-e3a2-4a9a-bc54-fd8d771b9a36",
   "metadata": {},
   "source": [
    "What a beautiful poem!\n",
    "\n",
    "Let's try the same with a very low temperature parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86fcbcf5-1e7c-462d-bcc3-13e02d3d762f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generation': \".\\n\\nThe normal distribution, oh so fair,\\nA curve that's symmetric, beyond compare.\\nIt's bell-shaped, with a peak so high,\\nAnd tails that fade, into the sky.\\n\\nThe mean, a noble knight, doth reign,\\nThe standard deviation, a fair maid.\\nThe data points, like soldiers in array,\\nFollow the knight, in a stately sway.\\n\\nThe normal distribution, a wondrous sight,\\nA pattern of data, that doth ignite.\\nIt's the bell-curve of probability,\\nA chart of the world, in harmony.\",\n",
       " 'prompt_token_count': 14,\n",
       " 'generation_token_count': 145,\n",
       " 'stop_reason': 'stop'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body = json.dumps({\n",
    "    \"prompt\": prompt,\n",
    "    \"max_gen_len\": 256,\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": 0,\n",
    "})\n",
    "\n",
    "response = bedrock_runtime.invoke_model(body=body, modelId=\"meta.llama2-13b-chat-v1\")\n",
    "response_body = json.loads(response.get('body').read())\n",
    "response_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec5f6b52-3170-42db-82f6-d96bdb6add42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "The normal distribution, oh so fair,\n",
      "A curve that's symmetric, beyond compare.\n",
      "It's bell-shaped, with a peak so high,\n",
      "And tails that fade, into the sky.\n",
      "\n",
      "The mean, a noble knight, doth reign,\n",
      "The standard deviation, a fair maid.\n",
      "The data points, like soldiers in array,\n",
      "Follow the knight, in a stately sway.\n",
      "\n",
      "The normal distribution, a wondrous sight,\n",
      "A pattern of data, that doth ignite.\n",
      "It's the bell-curve of probability,\n",
      "A chart of the world, in harmony.\n"
     ]
    }
   ],
   "source": [
    "print(response_body[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2495f6-df01-40d8-9b6f-e4f702bb0095",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
